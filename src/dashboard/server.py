import os
import threading
import traceback
import uvicorn
import numpy as np
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from typing import Dict, Any, List, Optional

from src.core.embeddings import get_embedding_service
from src.utils.logger import get_logger
from src.utils.config import get_config

logger = get_logger(__name__)

async def compute_semantic_edges(memory_nodes: List[Dict], top_k: int = 5, min_threshold: float = 0.1) -> List[Dict]:
    """
    Compute semantic similarity edges between memory nodes using embeddings.
    AGGRESSIVE MESHING: top_k=5, threshold=0.1 to force connections.
    """
    if len(memory_nodes) < 2:
        return []
    
    try:
        embedding_service = get_embedding_service()
        
        # Extract descriptions for embedding
        texts = []
        node_map = {}
        for i, node in enumerate(memory_nodes):
            desc = node.get("properties", {}).get("description", "")
            if desc:
                texts.append(desc)
                node_map[i] = node["id"]
        
        if len(texts) < 2:
            return []
        
        # Generate embeddings
        embeddings = await embedding_service.generate_embeddings_batch(texts)
        embeddings_array = np.array(embeddings)
        
        # Compute cosine similarity matrix
        from sklearn.metrics.pairwise import cosine_similarity
        similarities = cosine_similarity(embeddings_array)
        
        # Generate edges: For each node, connect to top_k most similar neighbors
        semantic_edges = []
        for i in range(len(similarities)):
            # Get similarity scores for this node
            sim_scores = similarities[i]
            
            # Find top_k most similar (excluding self)
            top_indices = np.argsort(sim_scores)[::-1][1:top_k+1]  # Skip self (index 0)
            
            for j in top_indices:
                similarity = float(sim_scores[j])
                if similarity >= min_threshold:
                    semantic_edges.append({
                        "source": node_map[i],
                        "target": node_map[j],
                        "type": "semantic",
                        "properties": {"similarity": similarity}
                    })
        
        return semantic_edges
        
    except Exception as e:
        logger.error(f"Failed to compute semantic edges: {e}")
        return []

app = FastAPI(title="Elefante Knowledge Garden")

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API Endpoints
@app.get("/api/graph")
async def get_graph(limit: int = 1000, space: Optional[str] = None):
    """
    Fetch graph data from pre-generated snapshot file (ChromaDB memories + Kuzu entities).
    The snapshot is generated by scripts/update_dashboard_data.py
    """
    import json
    from pathlib import Path
    
    try:
        snapshot_path = Path("data/dashboard_snapshot.json")
        
        if not snapshot_path.exists():
            logger.warning("Snapshot not found, returning empty graph")
            return {
                "nodes": [],
                "edges": [],
                "stats": {"node_count": 0, "edge_count": 0, "semantic_edge_count": 0}
            }
        
        with open(snapshot_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # Transform nodes to frontend format
        nodes = []
        for n in data.get("nodes", []):
            node_type = n.get("type", "memory")
            nodes.append({
                "id": n.get("id"),
                "label": n.get("name", "")[:50] + ("..." if len(n.get("name", "")) > 50 else ""),
                "type": node_type,
                "entityType": node_type,
                "properties": {
                    "description": n.get("description", ""),
                    "created_at": n.get("created_at", ""),
                    **(n.get("properties", {}) if isinstance(n.get("properties"), dict) else {})
                },
                "full_data": n
            })
        
        # Transform edges to frontend format
        edges = []
        node_ids = {n["id"] for n in nodes}
        for e in data.get("edges", []):
            src = e.get("from") or e.get("source")
            dst = e.get("to") or e.get("target")
            if src in node_ids and dst in node_ids:
                edges.append({
                    "source": src,
                    "target": dst,
                    "type": e.get("label", "RELATED"),
                    "properties": {}
                })
        
        # Compute semantic similarity edges for better visualization
        memory_nodes = [n for n in nodes if n["type"] == "memory"]
        semantic_edges = await compute_semantic_edges(memory_nodes, top_k=3, min_threshold=0.3)
        edges.extend(semantic_edges)
        
        logger.info(f"Loaded {len(nodes)} nodes, {len(edges)} edges from snapshot")
        
        return {
            "nodes": nodes,
            "edges": edges,
            "stats": {
                "node_count": len(nodes),
                "edge_count": len(edges),
                "semantic_edge_count": len(semantic_edges)
            }
        }
        
    except Exception as e:
        logger.error(f"Failed to fetch graph data: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/stats")
async def get_stats():
    """Get system statistics"""
    try:
        from src.core.orchestrator import get_orchestrator
        orchestrator = get_orchestrator()
        return await orchestrator.get_stats()
    except Exception as e:
        logger.error(f"Failed to fetch stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Serve Frontend
# We assume the frontend is built to src/dashboard/ui/dist
frontend_path = os.path.join(os.path.dirname(__file__), "ui", "dist")

if os.path.exists(frontend_path):
    app.mount("/", StaticFiles(directory=frontend_path, html=True), name="static")
else:
    @app.get("/")
    def index():
        return {"message": "Elefante Dashboard API is running. Frontend not found (run 'npm run build' in src/dashboard/ui)."}

def start_server(host: str = "127.0.0.1", port: int = 8000):
    """Start the dashboard server"""
    uvicorn.run(app, host=host, port=port, log_level="info")

def serve_dashboard_in_thread(host: str = "127.0.0.1", port: int = 8000):
    """Start the dashboard server in a background thread"""
    thread = threading.Thread(target=start_server, args=(host, port), daemon=True)
    thread.start()
    return thread

if __name__ == "__main__":
    start_server()
